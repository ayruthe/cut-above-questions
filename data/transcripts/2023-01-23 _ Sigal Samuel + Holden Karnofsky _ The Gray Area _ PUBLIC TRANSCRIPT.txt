PUBLIC TRANSCRIPT | Sigal + Holden Karnofsky | The Gray 
Area | 1/23/23
TITLE: Can effective altruism be redeemed?

SUBTITLE: How trying to do the most good went wrong

DESCRIPTION: 
Guest host Sigal Samuel talks with Holden Karnofsky about effective altruism, a movement 
flung into public scrutiny with the collapse of Sam Bankman-Fried and his crypto exchange, 
FTX. They discuss EA’s approach to charitable giving, the relationship between effective 
altruism and the moral philosophy of utilitarianism, and what reforms might be needed for 
the future of the movement.
Note: In August 2022, Bankman-Fried’s philanthropic family foundation, Building a Stronger 
Future, awarded Vox’s Future Perfect a grant for a 2023 reporting project. That project is 
now on pause.

Host: Sigal Samuel (@SigalSamuel), Senior Reporter, Vox
Guest: Holden Karnofsky, co-founder of GiveWell; CEO of Open Philanthropy
References: 
	"Effective altruism gave rise to Sam Bankman-Fried. Now it's facing a moral 
reckoning" by Sigal Samuel (Vox; Nov. 16, 2022)
	"The Reluctant Prophet of Effective Altruism" by Gideon Lewis-Kraus (New Yorker; 
Aug. 8, 2022)
	"Sam Bankman-Fried tries to explain himself" by Kelsey Piper (Vox; Nov. 16, 2022)
	"EA is about maximization, and maximization is perilous" by Holden Karnofsky 
(Effective Altruism Forum; Sept. 2, 2022)
	"Defending One-Dimensional Ethics" by Holden Karnofsky (Cold Takes blog; Feb. 
15, 2022)
	"Future-proof ethics" by Holden Karnofsky (Cold Takes blog; Feb. 2, 2022)
	"Bayesian mindset" by Holden Karnofsky (Cold Takes blog; Dec. 21, 2021)
	"EA Structural Reform Ideas" by Carla Zoe Cremer (Nov. 12, 2022)
	"Democratising Risk: In Search of a Methodology to Study Existential Risk" by Carla 
Cremer and Luke Kemp (SSRN; Dec. 28, 2021)
 
Enjoyed this episode? Rate The Gray Area ⭐⭐⭐⭐⭐ and leave a 
review on Apple Podcasts.
Subscribe for free. Be the first to hear the next episode of The Gray Area. Subscribe in 
your favorite podcast app.
Support The Gray Area by making a financial contribution to Vox! bit.ly/givepodcasts
This episode was made by: 
	Producer: Erikk Geannikis
	Editor: Amy Drozdowska
	Engineer: Patrick Boyd
	Editorial Director, Vox Talk: A.M. Hall 

	
INTRO:

Sigal Samuel: Hi, I'm Sigal Samuel…. sitting in today for Sean Illing.

music

CLIPS:
SBF is like the Jordan Belfort of the crypto era
SBF
SBF
SBF 
So yeah, he is facing some very serious charges 
In the early morning hours of November 11th it all came to an end. 
S…B…F. 
You may have heard those initials swirling around recently. They stand for Sam Bankman-Fried, 
a now-former billionaire who’s at the center of a huge scandal. 
This past November, his crypto exchange FTX imploded when he lost at least $1 billion in client 
money after covertly transferring the money to a hedge fund he owned. 
In December, he was arrested on charges of wire fraud, securities fraud, , money laundering, 
and more. He pled not guilty to all of them.
And this month… In January, he…launched a Substack…. which so far seems mostly about 
explaining how this whole crypto debacle happened. 
But crypto is not.. why I’m interested in SBF. 

I’m interested because of the major role he’s played in.. effective altruism — a newish social 
movement that’s all about using reason and evidence to do the most good for the most people. 
SBF was one of its brightest stars.. and biggest funders. Yet now…. it looks like he’s done a lot 
of bad to a lot of people. He’s obliterated the savings of countless customers. He’s also screwed 
over a lot of charities he’d promised to fund. 

And for transparency, I should note: In August 2022, Bankman-Fried’s philanthropic family 
foundation, Building a Stronger Future, awarded Vox’s Future Perfect a grant for a 2023 
reporting project. That project is now on pause.
So, this is a crucible moment for effective altruism. 
Its members had spent a lot of time thinking about how to do good in the world. They thought 
they’d found a great answer. Now, they’re rethinking their convictions… and they're asking 
themselves: Did the logic of effective altruism itself produce this bad outcome? Or was effective 
altruism just part of the scam? And…even more important…
Can the movement be redeemed?  
music

ID: I’m Sigal Samuel… and this… is The Gray Area. 

music

My guest today is Holden Karnofsky. He’s seen as a leader in the world of effective altruism, or 
EA for short. And his personal story kinda parallels the story of EA writ large.

To give you a preview, it goes like this:  the movement's evolved from one that's about helping 
people in the here and now… mainly poor people in poor countries with problems like malaria 
and intestinal parasites….  
To a movement that's pretty interested in the long-term future of humanity… like how to prevent 
our species from getting wiped out by rogue artificial intelligence.

I want to know – does Holden think effective altruism should go back to its original focus on 
helping people in the here and now — sort of EA 1.0?

OR does it still make sense to focus on the long-term stuff that typified EA 2.0? 

But most of all– given everything we've learned after the SBF scandal —  I want to find out: 
What should EA 3.0 look like? 

music

Sigal Samuel: Hi, Holden. Thank you so much for joining us on the show.
Holden Karnofsky: Yeah, thanks for having me.
Sigal: So when the SBF scandal broke on a scale from one to 10, how surprised were you with 
one being? Yep. This was entirely foreseeable, and 10 being I am completely flabbergasted. Oh 
my God, how the hell could this possibly have happened?
Holden: Uh, way on the high end of that scale. I don't know. We, we live in a crazy world. I don't 
know if I want to quite give it a 10, but I, I wasn't expecting it.
Sigal: Okay. I mean, effective altruism is a pretty tight-knit community. You know, there's 
regular conferences and meetings and conversations. So to what extent did you and SBF know 
each other? Were you friendly? Did you talk much?
Holden: I had met SBF I would say a handful of times. He was entering into a space that 
overlapped a lot with our space. So basically what happened was SBF was becoming the 
funder of a foundation that was funding some of the same causes and some of the same 
organizations we were. And that creates kind of a natural need to communicate a bit sometimes 
just say, Hey, we might both be interested in this grant.
Things like that. And so we did have periodic meetings by video chat. I met him a couple times 
at effective altruist events. We certainly weren't friends. It was a professional relationship with 
occasional conversations.
Sigal: What was your general impression of him? I'm just kind of curious, was your impression, 
this is a really scrupulous guy who sincerely wants to do a ton of good for the world or what?
Holden: My impression of a lot of people is agnostic. I really do try to reserve judgment. I think 
interviews are not very reliable signals of someone's character. It's hard to get a real grip on 
what someone's all about from talking to them. You have to see what they do. Yeah, I don't, I 
don't think I had a really strong read.
I mean, I do think there were signs of things to be concerned about with respect to SBF and 
FTX. He ran this Alameda research company and there were a number of people who had 
worked there who left kind of very upset with how things had gone down. And I did hear from 
some of them what had gone wrong and, from their perspective, what they were unhappy about.
And I did hear about things that really concerned me. Also just crypto, I think I've never been a 
crypto expert, but when I saw some of the advertisements for FTX, talking about crypto being 
very good for the world, or the Super Bowl ad, kind of saying, Hey, don't miss out on the next 
big thing. These are things in hindsight, I kind of wish had registered with me more.
But I, I'm not, I'm not gonna, you know, there were signs, there were things that made me 
nervous. There were things that made me say, okay, I don't really know what the deal is with 
FTX. I don't really know what the deal is with SBF, I certainly see some reasons that one could 
be concerned, that one could imagine just like low integrity behavior, less than honest and 
scrupulous behavior.
Behavior I'm not comfortable with at the same time. I just don't think that I knew anything that 
rose to the level of expecting what happened to happen or really being in a position to go 
around denouncing them or something. Honestly, they weren't someone who I thought of it as 
my job to vet or to have a really firm view on.
Um, now it feels a little bit different in hindsight, but it felt at the time, like he was a very famous 
person going around talking about effective altruism and associating himself with effective 
altruism. And it didn't really seem like there was much I could do in either direction about that.
Some of that does feel regrettable in hindsight.
Sigal: SBF kind of grew up on effective altruism. So when he was in college, he had this kind of 
fateful lunch with Will McCaskill, the moral philosopher who's probably the closest thing that EA 
has to a leader. And over lunch, SBF said that he wanted to devote his career to animal welfare, 
actually, but Will McCaskill convinced him he could make a bigger impact by choosing a career 
that would just make a ton of money and then donating all that money. 
So, you know, lo and behold, SBF pursues a career in finance and then crypto, and he starts 
pouring funding into all of these EA causes. But for you, I have the sense that the path was sort 
of different.
So tell me how you became involved with effective altruism, what your initial feelings were about 
it. Were there some aspects that kind of weirded you out?
Holden: Yeah, I, I did have a bit of a different path. Uh, my basic story is I, outta college, I 
worked for a few years in finance, worked at a hedge fund, worked at a couple hedge funds 
actually, and well, I wanted to give to charity and a bunch of my friends did as well. And we 
were having conversations and just talking about how we could sort of, in some sense get the 
best deal on giving to charity, help the most people for the least money.
This is a huge question. This is a lot of work. We're really interested in it. We're much more 
interested in this than we are in the bond market and we don't think we can do justice to this 
question part-time. So we quit our finance jobs, we raised money from our former coworkers, 
and we started GiveWell in 2007.
And no one was using the term effective altruism for years after that so GiveWell was just trying 
to solve a practical problem. 
And in 2012, I think it was, we met Cari Tuna and Dustin Moskovitz. Dustin is co-founder of 
Facebook and Asana, and they were looking to give away their personal fortune of billions of 
dollars and they were asking a similar question, Hey, how can we give this away and help 
people as much as possible with it?
Do the best thing with. And we said, Hey, we're really interested in this question too. But we felt 
that GiveWell was not the perfect thing for them because GiveWell was really aimed at the kind 
of person we were. It's like, Hey, I wanna give a few thousand dollars and have a few hours to 
think about it.
And Cari and Dustin are giving away, like, billions of dollars and had decades to think about it. 
We felt we needed a different kind of organization and workflow to get them the best 
recommendations. So we started Open Philanthropy originally as a project within GiveWell 
called GiveWell Labs, and then it spun out. And Open Philanthropy did go in a bit of a different 
direction and pursued this philosophy of hits-based giving, which is this idea that you might try 
10 different wacky things with your giving, and nine of them might go embarrassingly badly, and 
one of them might go amazingly well and make up for everything else.
And so we thought that was an interesting philosophy of giving to try out with Cari and Dustin, 
kind of researched the history of philanthropy and I felt surprised by how big some of the 
successes were, just in terms of the impact on issues like global poverty, feminism, and so that 
was kind of my trajectory.
And as the person running Open Philanthropy, I just became more and more professionally 
interested in this question of how do you do an outsized amount of good with money or with 
time? What are issues that are not only important but neglected? Intractable? So that you can 
get really far? I mean, I don't believe I was involved in, like, coming up with the term effective 
altruism.
It was these folks in Oxford who kind of came out with this brand and this name, and that was 
around 2013. So it's like we were kind of transitioning from GiveWell to Open Philanthropy at 
that point. And I had met a bunch of these people and thought they were interesting people to 
talk to. We definitely shared a lot of goals. And so when I learned about effective altruism, I said, 
Hey, doing the most good possible, helping people as much as you can, that's great.
That's something I'm interested in. I'm really interested in this. Yeah, there've always been 
things about effective altruism, the community and the brand and the idea that I've been less 
than comfortable with, but also, no one's perfect, no community's perfect. And I looked at this 
idea in this community and said, Hey, we, we have an awful lot in common in terms of what we 
are interested in accomplishing with our lives.
And, and that's absolutely still the case.
Sigal: I personally see you as a leader in the effective altruism world. Is that fair to say? Would 
you identify as that, or do you see yourself differently?
Holden: Probably a recurring theme that's gonna come up in this conversation is that it’s not 
like there's a CEO of effective altruism who decides what is and what isn't, and who is and who 
isn't. And so any question you ask like that, it's gonna depend who you talk to and who you ask.
Right. There's nowhere where I signed up to be an effective altruist. In fact, if someone asked 
me, you know, are you an effective altruist? Like, I don't always give the same answer. I mean, 
generally I'd say yes, but sometimes I would say, well, but that doesn't mean I agree with 
everything that comes out of the effective altruist community.
It's kind of a hard question to answer. Certainly it's the case that people who are interested in 
effective altruism often are reading what I'm writing and very interested in it, and things like that. 
I mean, that's definitely true.
Sigal: Right. So in terms of the aspects of effective altruism that maybe slightly weirded you out 
or you were slightly not super comfortable with, one thing that comes to mind for me, pretty 
obviously is utilitarianism. Let's start by establishing this. In the simplest terms to you. What 
does utilitarianism as an ethical theory entail?
Holden: So an important thing to lead off with, I'm not a philosopher, I don't have a PhD in 
philosophy, and one of the first things I noticed about the differences between me and the folks 
who are kind of coming up with the effective altruist name is that they were coming from 
philosophy tradition. A bunch of them were like actual philosophy professors.
Now I’ll say, I'm really interested in philosophy. I enjoy it a lot. I've read a bunch of it, I've argued 
about it a lot, but it's not my area of expertise. 
So I'd lead off by basically saying utilitarianism is roughly a theory that doing the most good 
possible, in some sense, taking all the benefit you gave to all the persons in the world – and I 
use “persons,” ‘cause it's not necessarily humans – if you take all the benefit you did for all the 
persons in the world and add it up and you do as much good as possible, utilitarianism would be 
the theory that that is what ethics is. 
And so therefore, if you are wondering whether you did the right thing, that is the same question 
as if you did the most good and that, I think — we can get into this — I think that is different from 
effective altruism and it's actually an idea that I don't personally subscribe to.
Sigal: Okay, great. So let's get into that in a second. But I just wanna say, you know, like this 
general idea of try to produce the greatest good for the greatest number, like try to maximize the 
overall good. At first, that sounds kind of nice, you know, but it can lead to a really weird ends 
justify the means kind of mentality.
There's one famous thought experiment that comes to mind for me, which is, If a patient shows 
up at a hospital, is it maybe the right thing to do to just cut him up and take all his organs since 
by killing him, you can potentially save the lives of five other people who need organs. Right? 
So the utilitarian, if they're pretty hardcore about their utilitarianism, might come and say, yes, 
cut up that one person, ‘cause you can save five other people.
And I'm just curious, you know, do you think that this style of thinking might have led SBF 
astray? As in, he might have thought, okay, it's fine to do this alleged fraud because he can 
make billions of dollars that way and then donate it all to amazing charities?
Holden: You know, I think the organs example, I do wanna say, I think your average utilitarian 
would give a different answer there. I think your average actual utilitarian would say, actually, 
no, that's not the way to do the most good. That would create a society where people are afraid 
to come into a hospital, blah, blah, blah, blah, blah.
So I think these debates can get a lot more nuanced and, and subtle than just, Hey, yeah, 
utilitarians are into grabbing random people's organs. I mostly don't endorse utilitarianism, but I 
also don't want to straw person it. 
So, you asked me could this have motivated SBF, I mean, I think there's a bunch of ideas here 
that kind of are sitting near each other, but are actually different ideas.
So there's utilitarianism, which is the idea that doing the most good is kind of all there is to 
ethics, or at least that's the very amateur definition I'm giving it. Then there's the ends justify the 
means, which can mean a few different things, but it might mean that you just kind of believe 
that you can do arbitrarily defective, deceptive, coercive, nasty things as long as you kind of 
worked out the numbers and they lead to a lot of good.
And then I think effective altruism is neither of those. It's a third thing which we can get to. So I 
mean, I don't know. I, like, honestly don't know if SBF was motivated by ends justify the means 
reasoning. I'm really going off of what I'm reading in the media. I don't have special insight and 
the media on this is confusing.
So like, got this weird interview where he says, ethics is all sham, but some people think he's 
just talking about certain flavors of ethics…
Sigal: That “weird interview” by the way was with Vox’s Kelsey Piper, and it was conducted, 
funnily enough, over Twitter DMs. We’ll put a link to that in the show notes.
Holden: It certainly looks like the actual ends that he caused to happen didn't justify and don't 
justify anything. And I'd be surprised if anyone would disagree with that.
It just like doesn't actually look like he did good. It looks like he did harm. So it looks like on 
utilitarianism or any other ethical system, it wouldn't, I don't think there's a story right here where 
what he did looks good, but is it possible that he was saying to himself, Hey, I'm taking these 
giant risks and it's totally worth it because I'm gonna make all this money and do all these great 
things with it.
It is possible that that's what motivated him. And I think that's a problem. I mean, that's 
something that bothers me. The fact that it's possible alone bothers me.
Sigal: Well, whether or not SBF personally was motivated by utilitarianism to have this ends 
justify the means mentality, and that's kind of what motivated this alleged fraud – beyond just 
the SBF question, thinking about EA generally, has EA leaned too hard into utilitarianism? I 
know that EA and utilitarianism are not one and the same.
But I think it's fair to say that there is like a pretty strong flavor of utilitarianism among a lot of top 
EA thinkers. And I wonder if you think that creates a big risk that members will be kind of likely 
to apply this philosophy in naive, harmful, ends justify the means kind of ways.
Holden: I feel like it is a risk and I wrote a piece about this called “EA is about maximization and 
maximization is perilous.” And this was back in September or something. So this was well 
before, any of this stuff came out. And I had no idea it was coming at that time. And, I said in 
this piece, I said, you know, here's something that makes me nervous.
EA is doing the most good. What does that mean? You're maximizing the amount of good, 
anytime you're maximizing something, that's just, it's perilous. Life is complicated and there's a 
lot of different dimensions that we care about, and we're often confused about what we 
ourselves want. So if you take some thing X and you say, to maximize X, you better really hope 
you have the right X.
And in fact, one of the classic concerns about AI risk is that you might have an AI that is trying 
to maximize something and it's not quite the right thing, and then we get a horrible catastrophe. 
And then also: maximize what? And I think we're all extremely confused about that. Even the 
effective altruists who are lifetime philosophy professors, I don't think there's a good, coherent 
answer to what we are supposed to be maximizing.
So I've talked about doing the most good, talked about doing the most good for our dollar. I 
think these are compelling, interesting ideas. They're vague ideas. They haven't been totally 
worked out. If you ask GiveWell, hey, what is it exactly that you maximized with my dollar? Was 
it live saved? Was it people helped?
They'll say kind of, well, it's complicated. We have this metric called the disability adjusted life-
year. We sort of use that, but we also apply some common sense. It's just kind of hard. So I 
think that is a dicey situation, and I just feel like you're always skating a little close to the edge.
It just feels like a current that we all have to swim against and a constant temptation to have 
trouble that we all have to deal with. And as far as I can tell today, I feel like we are dealing with 
it. I feel like when I meet actual effective altruists, they don't seem very ends-justify-the-means-y 
on the whole, with exceptions, they seem like high integrity people.
They seem like people who do believe in many different flavors of morality. They don't seem like 
the kind of people who think you should take someone's organs, but we gotta watch out. And so 
I wrote that and then this happened, and then I said, okay, maybe we have to watch out more 
than I thought we had to watch out.
Sigal: Right. I mean, I am glad you mentioned this blog post because, yeah, that was a couple 
months before the SBF scandal broke, and I think it was very prescient, especially that part 
where you say if you're maximizing X, you're asking for trouble by default. EA says it's about 
maximizing the good, but that's, like, something we're very conceptually confused about.
So this seems like it's gonna kind of set us up for trouble. And you say, by default it seems like a 
recipe for trouble. Do you kind of feel now that your warning was basically vindicated by what 
happened with SBF?
Holden: Do I feel vindicated? Uh, I don't know. I don't feel happy [laff].
Sigal: Yeah. But do you feel like, damn, I got that right.
Holden: I feel like the points being made in that post look, like, pretty right. Um, look like, Hey, 
we should be concerned about this. I feel more concerned about this than I did when I wrote the 
post for sure. Like, I feel like I should have listened to that post more than I did or something.
Sigal: Like you should have listened to your own inner voice?
Holden: Maybe, I mean, it's hard.
It's, it's, there's a lot going on at any given time and, and this is something that I've worried 
about for years. Knowing what I know now, I would've worried about it more. But all the worrying 
I do has costs because there's many things to worry about. And then there's many things to, to 
not worry about and to move forward with and do to try and help a lot of people.
Sigal: So are you kind of saying that maximize the good is a recipe for disaster?
Holden: Well, the thing I say in the post is that effective altruism, in my opinion, works best with 
a strong dose of moderation and pluralism.
Sigal: Mm-hmm.
Holden: I dunno, maybe this would be a good time for me to talk about what I see as the 
difference between utilitarianism and effective altruism. 
Sigal: Mm-hmm.
Holden: They both have this idea of doing as much good as you can.
Utilitarianism is a theory, it's a philosophical theory, and it says doing the most good – that's the 
same thing as ethics. Ethics equals doing the most good. So if you did the most good, you were 
a good person, and if you didn't, you weren't, or something like that. I think that it's something of 
that flavor.
And that's an intellectual view. You can have that view without doing anything. You could be a 
utilitarian and never give to charity and say, well, I should give to charity, but I just didn't. I'm a 
utilitarian because I think I should. And so that's utilitarianism. And effective altruism is kind of 
the flip of what I just said where it says, Hey, doing the most good, that's, I don't know, for lack 
of a better word: cool. We should do it. You know? We're gonna take actions to help others as 
effectively as we can. There's no claim that this is ethics. There's no claim that this is all there is 
to ethics. 
How I relate to effective altruism is, hey, doing as much good as you can: that's cool. I like it and 
I want to do it. That does not mean that is the only thing I want to do, the only thing I care about. 
It does not mean I think it's the same thing as ethics. So there's lots of times when I try to do the 
right thing and I'm not just calculating what's gonna do the most good. 
Being a good friend or telling the truth… these are not things where I'm diagramming out, how 
many utilons is this gonna create? These are things I'm trying to do ‘cause they're right. 
So this is the distinction, is, for me, effective altruism means: doing the most good. That's great. 
That's something we're gonna do. We're gonna take actual actions we're gonna give to charity, 
we're gonna give to the best charities we can, and there's this whole tangle of abstract, 
philosophical considerations that you do not have to sign on to to do that.
Now this is confusing and it's not exactly shocking if a bunch of utilitarians are very interested in 
effective altruism, and a bunch of effective altruists are very interested in utilitarianism. So, 
you're gonna get these two things. They're gonna be hanging out in the same place, and you 
are gonna face this danger.
I'm not terribly surprised that some people look at it and say, Hey, that is all I want to do with my 
life. I think that's a mistake. I think balance is important, but there are people who think that way 
and those people are gonna be drawn into the effective altruist community. And I get where 
that's coming from.
Sigal: The picture that I hear you painting… it kind of sounds like the way you're laying it out, 
EA and utilitarianism are not coextensive. They're not the same thing. There's maybe a bit of a 
Venn diagram thing going on here where EA and utilitarianism have a fair bit of overlap, but 
they're somewhat different.
I wanna put before you a slightly different possible way to read the EA movement. A few smart 
guys like Will McCaskill and, and some others, at Oxford especially, who wanted to help the 
world and give to charity basically looked around at the philanthropy landscape and thought, 
you know, this seems kind of dumb.
People are donating millions of dollars to their alma maters, like to Harvard or Yale, when 
obviously that money could do a lot more good if you used it to help, I don't know, poor people 
in Kenya. And they realized basically that the charity world could use more utilitarian-style 
thinking, but then they over-corrected and started bringing that utilitarian mindset to everything, 
and that over-correction is now more or less EA.
What would you say about a reading like that?
Holden: Uh, you know, I definitely think there are people who take EA too far, but I wouldn't say 
that EA equals the overcorrection. So I don't know. I mean, I kind of am just sitting here and 
saying, Look, I don't think utilitarianism, doing the most good is all there is to ethics. I don't 
consider myself a utilitarian, though, I think it's an interesting framework that has been very 
thought-provoking for me and that I give some weight. I give weight to a bunch of different 
ethical theories. 
You know, I'm not a philosophy professor. I don't think the ends justify the means. I have a lot of 
stuff I care about in my life other than doing the most good. I do tons of stuff that doesn't do the 
most good. 
All this is true and yet no matter how many things people do with cryptocurrency exchanges or 
something, I don't think it's ever gonna change me from thinking that helping as many people as 
you can by donating to the most effective charity you can is good and I want to do it. 
I feel like I don't want to let things get too complicated here and lose sight of that. And what 
effective altruism means to me is basically that. It’s, hey, let’s help a lot of people, let's be 
ambitious about helping a lot of people. Let's give to the best charities we can find. Not to 
whatever charity sent us a solicitation in the mail.
And gosh, that just seems really right to me and I really am passionate about it. And I give to 
charity every year and give to the best charities we can and, I don't know if I'm gonna call that 
an overcorrection. I feel like this is good.
And so, I think I'm more in the camp of this is a good idea in moderation. This is a good idea 
when accompanied by pluralism. Let's not throw the baby out with the bathwater, is kind of 
where I'm at.
Sigal: From your blog post about EA and maximizing, it sounds like you would like to see EA 
embrace a little more of the common sense moral rules, deontology vibe, right?
It seems like you would like to see more moral pluralism, if I could put it that way, a little bit more 
embrace of other moral theories, not only utilitarianism. Is that fair to say?
Holden: I do like to see that. I do want to emphasize, I don't think I'm alone here. I don't think 
that everyone who's excited about effective altruism is a full-on utilitarian except for me. I think 
the attitude I'm describing is actually fairly common, and I think a lot of the people I've met are, 
are in a similar boat to me.
Hey, let's help lots of people. That would be great. Let's cut back on some of the most 
dangerous issues facing humanity. That would be great. Let's not only donate, let's think about 
how we're using careers. That would be great, too.
I would like to see it. I do like to see it, but yes, I think I would like to see more of it. And it is 
something I've been thinking about is, is there a way to encourage or to just make a better 
intellectual case for pluralism and moderation, because I think it's, it's not just something that 
you get by asking for it.
music 

Is it more important to help struggling people right now, or to protect the 
long-term future of the human species? 

For a big philanthropic organization like Holden’s, this is far more than just 
a theoretical question.

I’ll ask him where he comes down on this issue of “longtermism”... after a 
quick break.

 

MIDROLL 1

music 


Sigal: So when you started out in your career, you were a lot more focused on very present day 
problems, like global poverty, global health, you know, the classic EA concerns, but then ea. 
Sort of how to pivot towards long-termism, which more or less is the idea that we should really, 
really prioritize positively influencing the long-term future of humanity, thousands, even millions 
of years from now.
Tell me the story of how you gradually became more convinced of long-termism.
Holden: I, I wanna draw some more fine-grained distinctions here.
Sigal: Sure.
Holden: Longtermism… yeah, I think has been expressed in a few different ways, but it tends to 
emphasize the importance of future generations. Generations other than this one. There's a 
separate idea of just like, I don't know what to call it, like catastrophic risk reduction.
So there's a separate idea that just says, Hey, you know what? There's some risks facing 
humanity that are really big that we really gotta be paying more attention to. One of 'em is 
climate change. One of 'em is pandemics. So Covid-19 was obviously an enormous tragedy that 
was one of the biggest world-changing events you can imagine.
I worry that as biotechnology advances and gets cheaper and easier to use, it could get easier 
for people to design viruses that are worse than Covid-19, that these could be done in a military 
context and could end up leaking out because I think the biosafety in these programs can be 
surprisingly terrible.
Then there's AI. And I've talked about AI and written about AI quite a bit. I think that the dangers 
of certain developments, certain kinds of AI that you could easily imagine being developed and 
we have no idea when they will be, are vastly underappreciated in my view. You could hold the 
view that AI risk and biorisk are huge issues that we should be paying more attention to without 
holding the view that we need to be obsessed with future generations.
You can also hold the view that we should consider future generations to contain most of the 
ethical value of the world without agreeing that biorisk and AI risk are the biggest risks to focus 
on. So, I would say that I'm currently more sold on biorisk and AI risk as, just, things that we've 
gotta be paying more attention to, no matter what your philosophical orientation. I'm more sold 
on that than I am on longtermism.
But I am somewhat sold on both. I've always kind of thought, hey, future generations matter. 
Future people are people and we should care about what happens to the future. I mean, that, 
that just seems like something I've always believed, and I've always been skeptical of claims 
that go further than that and might say something like, well, the value of future generations, and 
in particular, the value of as many people as possible getting to exist is so vast that it just 
completely trumps everything else, and you shouldn't even think about other ways to help 
people.
That's a claim that I've never really been on board with, and I'm still not on board with.
Sigal: So I'm glad you're bringing that up because this ties back to our earlier conversation 
about utilitarianism. I feel like that can be tricky enough when you're just looking at the world 
today, but when you add in the long-term future, it can just really go bonkers, right? And this is 
one of the common critiques of long-termism, that it's all about maximizing overall expected 
value, but that can lead you to pretty ludicrous conclusions. So, it sounds like that freaks you out 
and you feel like that might be going astray. Is that right?
Holden: Basically, yes. I think it, it, it can get a little subtle, I think if I knew for sure exactly what 
was going on and I had like perfect knowledge of everything, you know, that might be one thing, 
but I think in the real world, to go from this philosophical argument to like, therefore, you should 
be kind of ignoring all these real world problems and focusing entirely on things that might affect 
the long-term future.
Yeah, I don't buy that argument for a number of reasons. If it were not the case that AI risk and 
biorisk and, and other things like climate change looked like such a potential big deal this 
century – if it were the case that really all we could do about the long run future were this 
incredibly speculative stuff, reaching out, you know, billions of years and trying to figure out 
what's gonna happen then, I think I would just say like, this isn't very productive from a practical 
point of view.
We're not gonna get much done here. It's better to like, do things we can do and maybe learn 
about the world as we do them. So I think there's a lot of reasons that I wouldn't be into that. I 
also think that like the rabbit hole goes deeper. So this is a thing that… you can keep going 
down this road. So you can say, well, you know, if you wanna maximize your expected value, 
you should really focus on the people in the future.
But what about people in other parts of the multiverse? What about people in other parts of an 
infinite universe who you might be able to affect through your actions being correlated with 
theirs through various theoretical mechanisms tied to some of these like non-causal decision 
theories I mentioned. 
What about the fact that if there's actually infinite people in the universe that all of this stuff is 
undefined and you can't take an expected value over anything, that there can't really be an 
ethical basis for doing one thing or another at all? More or less? Unless you take on some other 
weird assumptions in your ethical system that break a bunch of other stuff and cause a bunch of 
other problems – I actually could go on like this for, like, quite a long time.
Sigal: This is kind of what I meant when I said if you go further along this line of thinking, it just 
goes really bonkers. So without going into infinite worlds and all of that fun craziness, just to like 
really show that this is a very concrete issue you're probably dealing with every day when you 
show up at work.
This comes up in your work at Open Philanthropy because you guys have millions of dollars to 
give away. You have to decide what to spend it on. And, if you say, for example, I think there's a 
0.0001% chance trillions of future humans will never get to exist because rogue AI wipes out all 
of humanity – well, then if you're just focused on maximizing expected value, You might decide 
to just spend all your money on AI safety and $0 on helping people who are poor today, but like 
that seems weird. So at Open Phil., you guys use a method that's called worldview 
diversification to decide how to divvy up money to different causes.
So, tell me how worldview diversification works.
Holden: Yeah, I mean the basic idea of worldview diversification is that – it kind of comes back 
to this vagueness of the question, what does it mean to do the most good? So we're gonna try 
and do the most good with our money. And on one hand, I think there are times when it looks 
fairly clear that some things do more good than others.
Giving to the Against Malaria foundation to distribute anti-malarial bed nets in Africa so that 
fewer children die of malaria, fewer people get sick versus giving to my alma mater, Harvard. I 
think that the malaria one does more good and I, I don't feel very conflicted about that. I think 
that's a call I'm fine to make.
And then, though, if you wanna get all the way into it. What does it mean to do the most good? 
can get very complicated and can get very hairy, and you start to have these different ways of 
looking at the questions. So that's where the term worldviews comes in. I call them worldviews. 
They're intellectual frameworks that have radically different conclusions.
So you can have an intellectual framework that says, what we're about here is we're about 
estimating the number of utility-translated benefits to people and maximizing that. And if you 
make your best guess at a bunch of probabilities, the result is gonna be that you should put, 
like, every dime into reducing existential risk and you should not give anything to helping people 
in poverty.
And then there's like another worldview that says, kind of, I don't really reason by drawing out 
expected value diagrams and putting best guess probabilities on everything, and using this 
technique that I've called Bayesian mindset, that's not how I reason, and all this stuff is very 
speculative and I don't trust it, and I'm actually interested in doing something that's more robust, 
more evidence-backed within that, trying to maximize and do as much good as I can and help 
the most people possible, but I'm restricting myself to things that I pretty much understand and 
pretty much have evidence behind them. 
And then there's other worldviews that could come in and you could say, well, but I also feel 
really strongly that animals should be getting  some kind of competitive consideration to humans 
in terms of how much we should care about them.
That might lead you to say, you should put all your money into, for example, corporate 
campaigns to help ensure that animals are treated better on factory farms, or that we hopefully 
eventually get rid of factory farming. Open Philanthropy does a ton of work on that. 
So you have these different worldviews. So it's like, if you take this set of assumptions or this 
framework or this way of thinking, it'll tell you to put all your money over here. If you take this 
other way of thinking, it'll tell you to put all your money over there. And to me, the best metaphor 
is it feels like it's these different Holdens arguing with each other. There's these different pieces 
of myself that are all trying to reach a deal. 
And in my view, a very natural way to handle that kind of dispute, if you have a bunch of 
different people who think in completely different ways, they're not speaking the same language, 
they're not using the same numbers, they don't even agree on what the numbers mean, what do 
you do? You divide up the resources. You say, hey, there's three of you. Maybe you each get a 
third.
Or maybe you say, hey, there's three of you. One of you is making more sense to me. And so 
maybe one of them gets 60% and two of them each get 20%. But you divide it up and then you 
have these different worldviews that have their own budgets. And so it's like you could have the 
philosophy-driven Bayesian mindset bucket and you could have the practical, evidence-driven, 
human-focused bucket.
And they're each maximizing how to do the good, the way that they do it, the best way that they 
can. They could even conceptually make trades with each other or something like that. That's 
worldview diversification; it's just the choice to say, Hey, we're not gonna be stuck with one 
worldview and put all the money into one kind of thing. We're going to take these different 
worldviews that tempt us and divide us and divide up the money between them. 
I should also say, I think this is kind of a naive, obvious thing to do when you have different 
parts of yourself arguing with each other. Then there's a bunch of arguments and it's actually 
wrong, and you actually should just put all the money into one thing and maximize expected 
value, and then there's a bunch of arguments against that, and that goes down a very deep 
rabbit hole.
But my view is actually that the first intuitive thing is about as good as anyone's come up with.
Sigal: Personally, I'm very glad that Open Philanthropy does worldview diversification. It just 
seems wise to kind of hedge your bets intellectually that way, and to not become so single-
minded about just maximizing one thing, whether that's animal welfare or future people coming 
to be alive in millions of years or whatever.
And this has me just thinking about how, in EA circles, there's this fear that's commonly talked 
about: the idea that we’ll inadvertently design AI that is a single-minded optimizing machine. 
And doing whatever it takes to achieve a goal, but in a way that's not necessarily aligned with 
values that we approve of.
And so the paradigmatic funny example here is we design an AI and we say your one function is 
to make as many paperclips as possible. We wanna maximize the number of paperclips. And 
we think like, okay, cool, that's a fine goal. AI will do a fine job of that. 
But then, the AI doesn't have our human values. It's just an AI program to do one thing, and so it 
goes and does crazy stuff to get as many paperclips as possible. And that could mean, 
colonizing the world and the whole universe to gather as much matter as possible and turn all 
matter, including people, into paperclips, right? Like it could really go crazy.
So that's just a bit of a tongue in cheek example, but I think this core fear that you hear a lot in 
EA about a way that AI could go really wrong if it's a single-minded optimizing machine. Do you 
think that some effective altruists have basically become the thing that they're scared AI will be? 
Single-minded optimizing machines?
Holden: It's interesting. I mean it's an interesting kind of thought experiment and I think it may 
be a bit of projection, like there might be some people in effective altruism who kind of are trying 
to turn themselves from humans into ruthless maximizers of something and they may be 
imagining an AI would do the same thing.
I should say. I mean, my story about AI risk is like, kind of pointed in a similar direction. It has a 
very different flavor. I would just say, hey, like, humans being as powerful as we are and being 
able to develop the technologies we do has been a little bit of rough news for other animals. 
We've driven a lot of other species extinct, not because we're like single-mindedly trying to 
maximize the amount of, I don't know, babies we have or sugar that we eat, but just because 
we're powerful and there's things that we want and we want those things more than we want to 
be nice to the animals a lot of the time.
And I think we should worry about AI being the same with respect to us, that I don't think you 
need some kind of weird God in a box to have a problem with AI. I've written about how if you 
just take it down a notch, just assume you built an AI that's basically able to do what a human 
could do. Some different strengths of weaknesses, but the same basic idea could do what a 
human could do. Then just like is, is a little bit different, just wants different things, has different 
values that doesn't have the same ethical rules that we have. 
If you imagine that, that's plenty scary because if you just had something like that, that could 
make copies of itself, that alone, you could get disaster from that. You do not need this weird 
alien psychology that's maximizing paperclips. You just need to think of it as, once there's what 
you could call a second advanced species on earth, another set of minds that is able to build its 
own technology like humans are, but is different from humans, wants different things from 
humans. We could actually be in a lot of trouble there and, and you don't need these exotic 
psychologies to get the problem.
Sigal: I'm glad you used the word projection in terms of the question of whether EAs have 
maybe projected onto AI, uh, a little bit of what they themselves might be doing. I think there 
might be a little grain of truth there.
Holden: I think in some cases, and I think not in other cases. But I think in some cases, yeah, it 
could be happening.
music 

In the wake of the SBF scandal, a huge question hanging around effective 
altruism is this: 

What will the future of this movement look like?

And that’s exactly what I’ll ask Holden…. after one last quick break.

 

MIDROLL 2

music 

Sigal: Let's think about this. What reforms do you wanna see in EA, if any? What would it look 
like for EA to seriously engage with these critiques we've been talking about, and then use them 
to become more sound, both as a philosophy and as a social movement?
Holden: So it can be a little harder to talk about reforms, again, because it’s not like a top-down 
thing. There aren’t existing rules, or like a place where you sign up and become a member. And 
so it's hard to kind of talk about changing them.
But I guess a couple things I've thought, I mean, one is I definitely upgraded my sense of how 
concerned we should be about the proximity of these ideas, about the idea of, hey, it's cool to 
do a lot of good. And then the idea of hey, anything can be justified as long as you are reducing 
the probability of these, like, wild events that we talk about. These are two separate ideas. I'm 
into the first one, not into the second one, but it's not shocking that the two, you know, they're 
standing next to each other and, and it can be a problem.
It's not really obvious what to do about that. But I think there are some things. One is I do think 
there could be more effort to actually just make the intellectual case. And a lot of people who 
are excited about effective altruism, they're not going to respond to people kind of saying, Hey, 
I'm offended. Please change. They're gonna respond to arguments. 
And I actually think the arguments are there. I think if you go further down the rabbit hole, it's 
like this first blush, you know, hey, utilitarianism, let's do the most good. That means it's all 
about the future generations. If you keep going down the rabbit hole, I think you can make a 
pretty good case for coming out the other end and saying, We are too confused and uncertain 
and lost to be all in on this framework, and actually doing some kind of moral parliament or 
worldview diversification is probably the thing to do – actually on more than one line of argument 
and on a bunch of different grounds. 
So, I think there could be more effort put into that than there has been. Unfortunately, the 
philosophy literature on moral uncertainty is pretty small. There hasn't been a ton written about 
it, and I think there could be more written about it. So I think that's something I'd like to see more 
of.
Sigal: I wanna pick up on that, because for me personally, I see a little bit of tension between 
two things I've seen from you. On the one hand, you know, you sound like you really have this 
embrace of moral pluralism and you, I think correctly, want to see different moral theories taken 
seriously, appreciated for the moral intuitions that are actually genuinely important maybe, and 
they're capturing something real. We should pay attention. 
And you are well aware of and have pointed out and written about how a view that encourages 
us to maximize just one thing exclusively can be really risky and perilous. I've also seen you 
write about how you're bothered by ethics that's based on common sense intuitions and societal 
norms, because that does have a terrible track record like, people used to think that slavery was 
ethically fine. 
Holden: Yup. 
Sigal: And so you have developed this idea of future-proof ethics. So, a theory that'll stand the 
test of time, so that we're acting now in ways that people would still approve of in, let's say a 
hundred, 200 years. But to me, that requires some clear moral rules or some systemization.
And it seems to me like any systemizing moral theory, whether it's utilitarianism or something 
else, is going to encourage us to maximize one thing exclusively, and if we want to not get into 
trouble with that, we maybe have to give up on the seductive dream of finding one tidy rule or 
clear cut thing that we're always just optimizing for.
And I wonder if you feel ready to bite that bullet.
Holden: Well, you're, you're saying that, you know, I've got contradictions and tensions. And 
that's exactly how I’d put it. And that's, that's the situation. I think that I, I simultaneously am 
saying to myself, hey, on one hand, I think that going down this track of being all in on 
utilitarianism and being a super maximizer is really dangerous.
And on the other hand, just going along with what everyone around me is saying, and what's 
kind of in to think about what's right, also looks horrific. It looks horrible. Historically, it looks like 
such a terrible move. 
So I actually do vibe the goal of utilitarianism, as a lot of people practice it, which is to, Hey, let's 
be kind of scientific about it. Let's be systematic about it. Let's get ahead of the curve. Let's run 
some numbers. Let's think of the things that we'll think of later, and think of the things that we 
would think of if we had more reflection and wisdom. And that leads you down to science-y, 
maximizing direction. And then there's the moderation that leads you down a, hey, let's not get 
too carried away direction. 
And you're like, pick one, and I'm like, no, I'm not going to pick one. It's both. 
I think utilitarianism has a lot to recommend it. I think it's a compelling philosophy and I think 
there's a lot good about it. I think I've become more reasonable and smarter about morality and 
more thoughtful about morality because of my discussions with utilitarians and my consideration 
of utilitarianism, and there is part of me that is utilitarian for sure. So I think that's just what it is. 
I mean, what I am trying to do myself and what I would love to make arguments to other people 
interested in effective altruism to do, is to just live with that tension and just say, look, I'm a 
human being. I've got multiple voices arguing in my head all the time. I've gotta live with that. 
And we're gonna get along the way that any group gets along, we're going to try not to do 
anything that any one part of me totally hates too much. And we're gonna try to do all the things 
that any one part of me is super excited about as long as they're not totally unacceptable to the 
other parts, and just like a family or a team, that's how we're gonna get through life. So I, I think 
that is actually how I think of it.
Sigal: I mean, I think another way to read this ambivalence is to read it as an indicator that this 
whole endeavor of science-y maximizing for ethics is, is kind of a category mistake. And that 
this sort of systematizing science-y language of making something future-proof in terms of 
ethics really relies on a kind of moral realism, right? It’s the idea that there are objective moral 
truths and…
Holden: That's not really where I'm at, for what it's worth.
Sigal: Okay.
Holden: I've tried to lay out a vision for moral progress that doesn't rely on moral realism. So I'm 
personally not a moral realist. I don't think they're objective moral truths. And yet when you say 
to me, hey, was it just fine and the same, for example, the way that homosexuality used to be 
treated in society, was that fine and the same as now?
And I'm like, no, now is better. And you're like, what do you mean by better? Is it objective moral 
truth? I'm like, no, but it's better. 
The future-proof idea is me trying to square that circle and say, actually you don't have to 
believe in objective moral truths to believe in moral progress. The idea of future-proof ethics is 
kind of trying to get ahead of what we would think if we were wiser and did more reflection, 
which is different from finding an objective moral truth.
Sigal: I mean, I think personally the way I square the circle is to just acknowledge that the very 
notion of moral progress, and like what is morality, is to some degree historically contingent, 
culturally conditioned, and I'm fine with that. But I know that your mileage may vary.
Holden: Yeah, I, I don't think it's a category error, I don't think it's worthless, I don't think it's 
futile to talk about getting ahead of the moral curve. I just don't want to go crazy and go all in on 
it. And that's just me living with the contradictions. 
Another analogy would be like, in baseball, there was kind of a moment where people went from 
evaluating players by watching them play to evaluating them by crunching a bunch of numbers, 
and what was the right answer? And it's just like: the right answer was always both. Today's 
baseball teams are just using both. 
And so I think that's where I'm at, is I'm just like, I don't want to just stick with conventional 
morality. I don't, and I don't want to reject utilitarianism overall, I think it has insights and things 
to offer us, and I don't wanna be all in on it until we have the full truth and full confidence, we're 
just gonna have to mix our system, and put 'em together in one place and live with that.
Sigal: I do wanna get to some other more concrete questions here about the future of EA. One 
of them is about funding. I think that's a pretty important one. A bunch of EA's critics had warned 
that the movement's funding needs to be more decentralized. I know that the Oxford scholar, 
Carla Cremer, for example, she argued that the movement should allow for more bottom up 
control over how the funding is distributed, and actively fund critical work.
Do you agree that EA needs more of that?
Holden: I am very in favor of critical work of EA being welcomed and accepted and funded. And 
I know there's a lot of jokes about EAs being sort of obsessed with being criticized and, you 
know, loving to be criticized. And I think that those jokes are sort of true, but I mean, I think 
that's a good thing. I think it's endearing.
So I, I support that. I think when it comes to diversifying funding, I think it would be a lot better if 
EA had more diverse funding. And it's been unhealthy for Open Philanthropy to have such a 
large share of the funding for some of the causes that effective altruists work on. It's not a great 
situation. I'm not sure exactly what to do about it. So FTX actually, the FTX Foundation run by 
people who, as far as I know, I have no particular reason to think were involved in the FTX 
company practices. You know, they had a re-granters program, so they were actually taking, 
you know, a large number of people who were not at the foundation and saying, Hey, you can 
give away our money for us.
And in some ways that's a diversifying move, but does that really change the power dynamics? 
Does that really spread it out? It all is still coming from the one source. And so I'm not sure I 
know literally how to… like, I think it's bad. I think it's bad that there's one funder that has so 
much of the money.
I'm not sure that I have a solution in front of me, and I think that is kind of a tough situation. I 
think the best solution would be, uh, if there's any billionaires listening, you know, hey, there's a 
lot of great work that I think could use more funding. And if there were more people providing a 
lot of the funding, there would be less pressure pointed at any one.
Sigal: I mean, I think another way into this is talking about different kinds of diversity here and 
cultivating that even within these organizations that do have currently a lot of the money and 
power and you know, I think it's fair to say that so far EA has been largely a white men with 
fancy college degrees kind of space. 
And so one of the structural reforms that's been proposed for EA is to cultivate more intellectual 
diversity and to really democratize how the ideas get evaluated instead of just relying on this 
very over-centralized kind of power structure where a few very elite and unrepresentative voices 
are the ones that get heard.
Do you think that that's on point?
Holden: Well, I definitely think it would be good for EA to be moving in a more diverse direction 
and there are a lot of efforts to do that. And I want them to continue. We're not where we want 
to be on diversity, and I wanna see that get better. It is going to be tough to avoid a fundamental 
dynamic that at the end of the day there is a small number of people with a lot of money and it's 
their money and they're the ones deciding where the money goes.
Sigal: To sort of make this a little bit more concrete and think about what it could look like in 
practice to take this on board. So the scholars Carla Cremer and Luke Kemp have thought 
about this and they've advocated for effective altruists to use, maybe more deliberative styles of 
decision making. So something like citizens assemblies, where you get a group of randomly 
selected citizens.
You present them with facts, they debate, and they arrive at a decision together. We've already 
seen that kind of assembly in the context of deciding climate policy and abortion policy. Would 
you like to see EA be democratic in a similar way?
Holden: I definitely think it's an interesting thing to experiment with, and I think there could be 
promise there. I think going from idea to practice is hard, and I think you're going to have all 
kinds of questions about where does this bottom out? Who is setting the rules of the discourse? 
Because we have something where everyone gets a voice and it is representative, and that's 
our government, that's our democracy.
And so if you aren't wanting to send your money there, and having it spent the way it gets spent 
there, you're gonna have to make some kind of distinction that we're doing this based on a 
different set of values, a different mission, a different set of people. 
Who are those people? Who counts? Who's voting on where the money goes? How are they 
voting? Who gets how many votes? How long are they deliberating? And the people structuring 
that are gonna have a lot of power in a way that may be a little hard to understand, and it is still 
all gonna trace back to the people that have that money and choose to part with it.
So I think the challenges are there, but I think it's also interesting and, and there could be 
experiments in that direction.
Sigal: You know, EA isn't just a pure philosophy in the ivory tower, even though we've spent a 
lot of time talking about utilitarianism, et cetera. It's also, I would say an ideology, it's a social 
movement. It's a subculture, it's an identity. What do you wanna say, to the many young 
effective altruists who kind of built their whole identities around EA in some cases and really 
took EA ideas as gospel, and are now feeling maybe kind of let down and shaken in their faith 
post-SBF?
Holden: Well, I, I, I'm not really a fan of taking ideas as gospel. I really wouldn't be happy with 
myself or with others in EA kind of taking anything about EA on faith. What was the other term 
you used? There was something else I hope people aren't doing. Uh, oh: building their whole 
identity around it.
Sigal: You see that though, right?
Holden: I, I do, I do.
Sigal: You see that among, especially some young —
Holden: Yeah. I think it's an issue, and I try to push back against it and I'll try and do so now. I 
mean, I think I'll say in, in closing, I'll say that, you don't have to build your whole identity around 
EA in order to do an enormous amount of good, in order to be extremely excited about effective 
altruist ideas.
If you can be basically a normal person, in most respects, you have a lot going on in your life. 
You have a lot that you care about, and you have a job, and you work about as hard as, as a lot 
of people work at their jobs. You know, I'm not trying to do a lot of good by working especially 
hard or by being more self-sacrificing than other people.
And it's like, if you can just clear that first hurdle and be kind of a multi-dimensional, healthy 
human being who's just working hard the way a lot of people work hard, you can do a huge 
amount of good, you can help a huge amount of people. 
And then that next step where you build your whole identity around it, you get rid of everything 
else in your life. You say, this is all that matters, and you'll do anything. That, I don't think it's 
adding very much. It may be adding negative. It may be making things worse by tempting 
people toward the sort of behavior that we're worried about. You know, I do have concerns 
about people working too hard, stretching too hard, making it their whole identity, burning out, 
going the opposite direction, and I do think you could do an enormous amount of good without 
doing any of that and adding all that dicey stuff on, getting you very marginal gains and, and 
very, in my opinion, very possibly getting you negative results so….
You work hard and you do your best in the way that a lot of people do their best. It's a very 
unjust world out there, and you can do a lot of good, you can help a lot of people, and I think 
that is an idea that is core to effective altruism. It is really important and that, I think ultimately, I 
hope to see that idea gain in standing, not fall in standing, as time goes on.
Sigal: My one gripe with that would be, I don't know if it's quite fair to say that's the core idea to 
effective altruism, because it's not just, and we've talked about this, but it's not just the simple 
phrasing of “do a lot of good. That's cool.” It's “do the most good possible,” and I suspect that 
that being built into the foundations might be what drives a lot of particularly young, idealistic 
people to feel like they need to push it, push it, push it to the max, right? 
Because “push it to the max” is built into the DNA of the ideology, whereas I, what I actually 
hear you saying is something far more nuanced. I like that nuance and that nuance is something 
I would hope to see prevail. 
Holden: Well I would say, “do the most good possible” I think is a good idea in moderation. That 
might sound kind of like a contra–, but it's similar to, you know, running: a faster time is better, 
but you can do that in moderation. You can care about other things at the same time. I do think 
there is a ton of value to coming at doing good with a mindset of finding the way to do the most 
good I can with the resources I have.
I think that brings a ton of value compared to just trying to do some good. But then doing that in 
moderation I think does get you most of the gains and is ultimately where I think the most 
healthy place to be is. And probably, in my guess, probably the way to do the most good in the 
end, too.
Sigal: Thank you, Holden. I really appreciate you being in this conversation with me. It's really 
great to get to talk about these ideas with you.
Holden: Cool. Yeah. Likewise. It was a great conversation. Thanks for having me on.
music 

OUTRO/CREDITS

Erikk Geannikis [juh-KNEE-kiss] is our producer; Amy Drozdowska [drose-DUFF-
skuh] is our editor, Patrick Boyd engineered this episode, Alex Overington wrote our 
theme music, and A.M. Hall is the boss. 
Let us know what you think. Drop us a line at the gray area at vox dot com.
And if you appreciated this episode, please, share it with your friends. Send it to all the 
people in your life who are trying to find the best way to good in the world. You got 
people in your life like that, right?
I’m Sigal Samuel, I write about artificial intelligence and other big scary things on Vox 
dot com. Feel free to check me out there.  
Sean Illing will be back on Thursday with a new episode of The Gray Area.

Listen and subscribe.
1
